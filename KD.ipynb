{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7d58c032-c339-42bb-acf5-a8f3a3579f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rich\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from safetensors.torch import save, save_file, load_file\n",
    "from copy import deepcopy\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW, Adam\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "from lightning.pytorch.callbacks import RichProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cea4d4-d1ed-4f5f-b1ce-2ca22e92d085",
   "metadata": {},
   "source": [
    "# Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e3e955-8f37-46d6-9222-859a296bae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7323591e-fd91-4d12-8f06-5a29b08fc43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5eb704-ed41-40fb-8dd7-4090e4dd1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dac8058-abef-4b0c-a5b3-304afe061c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = dataset['train']\n",
    "data_dev = dataset['validation']\n",
    "data_test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5724d72-f24c-40d4-96b3-638a12f4c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x, tokenizer):\n",
    "    return tokenizer(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b567f9-714d-4e24-8776-2d379f0ac35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.map(lambda x: tokenize(x, tokenizer_bert), batched=True)\n",
    "data_dev = data_dev.map(lambda x: tokenize(x, tokenizer_bert), batched=True)\n",
    "data_test = data_test.map(lambda x: tokenize(x, tokenizer_bert), batched=True)\n",
    "data_train = data_train.remove_columns(['text'])\n",
    "data_dev = data_dev.remove_columns(['text'])\n",
    "data_test = data_test.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c2f171-15f2-411a-a1c9-1dae92404d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7d06f-f946-476c-99de-ab4ff98e12cf",
   "metadata": {},
   "source": [
    "## Define the teacher model\n",
    "\n",
    "We use Pytorch Lightning here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7cea3eec-9d1d-499f-bdb7-9bcdf3a84d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model, lr: float):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.ce_loss = CrossEntropyLoss()\n",
    "        self.train_f1 = F1Score('multiclass', num_classes=6, average='macro')\n",
    "        self.val_f1 = F1Score('multiclass', num_classes=6, average='macro')\n",
    "        self.test_f1 = F1Score('multiclass', num_classes=6, average='macro')\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        y = batch.pop('labels')\n",
    "        y_hat = self(**batch).logits\n",
    "        loss = self.ce_loss(y_hat,y)\n",
    "        self.train_f1(y_hat, y)\n",
    "        # self.log(\"f1_step\", self.train_f1, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y = batch.pop('labels')\n",
    "        y_hat = self(**batch).logits\n",
    "        loss = self.ce_loss(y_hat,y)\n",
    "        self.val_f1(y_hat, y)\n",
    "        self.log(\"f1_val_step\", self.val_f1, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y = batch.pop('labels')\n",
    "        y_hat = self(**batch).logits\n",
    "        loss = self.ce_loss(y_hat,y)\n",
    "        self.test_f1(y_hat, y)\n",
    "        self.log(\"f1_test_step\", self.test_f1, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a6cfa8e-7dd0-405e-8ec1-d68070114f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_teacher_model = FTModel(model_bert, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c48009e4-bd28-4ae3-82a0-b90dadb3b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(data_train, batch_size=64, collate_fn=DataCollatorWithPadding(tokenizer=tokenizer_bert), shuffle=True)\n",
    "dev_dl = DataLoader(data_dev, batch_size=64,  collate_fn=DataCollatorWithPadding(tokenizer=tokenizer_bert))\n",
    "test_dl = DataLoader(data_test, 64, collate_fn=DataCollatorWithPadding(tokenizer=tokenizer_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "581edf32-858d-4a81-b5ba-d7bbbc7a8f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3868, -0.0174,  0.2346,  0.2838, -0.4430, -0.0506],\n",
       "        [-0.6515, -0.4207,  0.3611,  0.3695, -0.0037,  0.3715],\n",
       "        [-0.4106, -0.2959,  0.4098,  0.2875, -0.0127,  0.1310],\n",
       "        [-0.3918, -0.2167,  0.2953,  0.3553, -0.1297, -0.0403],\n",
       "        [-0.4535, -0.1855,  0.3604,  0.2629, -0.2811,  0.1525],\n",
       "        [-0.5081, -0.4296,  0.4088,  0.3135, -0.0986,  0.3215],\n",
       "        [-0.3434, -0.1394,  0.2728,  0.3082, -0.3253, -0.0564],\n",
       "        [-0.4396, -0.2799,  0.3979,  0.2847, -0.0443,  0.1602],\n",
       "        [-0.3713, -0.2373,  0.3243,  0.2871, -0.2001,  0.0391],\n",
       "        [-0.5784, -0.4244,  0.3802,  0.2692,  0.0708,  0.2732],\n",
       "        [-0.5308, -0.3879,  0.3763,  0.3239, -0.0937,  0.1885],\n",
       "        [-0.4805, -0.0769,  0.3745,  0.3049, -0.3659,  0.0009],\n",
       "        [-0.5932, -0.2624,  0.3935,  0.3139, -0.1337,  0.1156],\n",
       "        [-0.3756, -0.1220,  0.3192,  0.2737, -0.4083, -0.0438],\n",
       "        [-0.3672, -0.0963,  0.3253,  0.2101, -0.2910, -0.0528],\n",
       "        [-0.6309, -0.4046,  0.3924,  0.3427, -0.0008,  0.1630],\n",
       "        [-0.3268, -0.1047,  0.3623,  0.2569, -0.3455, -0.0271],\n",
       "        [-0.4951, -0.4213,  0.4351,  0.2890,  0.0537,  0.1946],\n",
       "        [-0.4032, -0.0463,  0.3019,  0.3697, -0.5218, -0.0068],\n",
       "        [-0.3785, -0.4128,  0.2690,  0.3679, -0.0486,  0.1785],\n",
       "        [-0.5520, -0.3578,  0.4689,  0.3169, -0.0607,  0.1991],\n",
       "        [-0.2755,  0.1074,  0.2062,  0.1951, -0.4930, -0.1621],\n",
       "        [-0.6927, -0.4592,  0.3628,  0.4191,  0.0223,  0.4082],\n",
       "        [-0.3642, -0.2209,  0.2867,  0.1869, -0.1964,  0.1082],\n",
       "        [-0.4054, -0.0616,  0.2530,  0.2795, -0.4059, -0.0868],\n",
       "        [-0.6044, -0.3562,  0.4039,  0.3181, -0.1076,  0.2887],\n",
       "        [-0.3918, -0.0801,  0.2887,  0.2856, -0.4030, -0.0499],\n",
       "        [-0.3124,  0.1273,  0.2048,  0.2209, -0.4679, -0.1061],\n",
       "        [-0.4627, -0.4695,  0.3675,  0.3064,  0.0388,  0.2061],\n",
       "        [-0.5854, -0.2851,  0.3721,  0.3548, -0.1023,  0.2195],\n",
       "        [-0.4214, -0.3925,  0.3561,  0.2365, -0.0728,  0.1293],\n",
       "        [-0.5101, -0.3558,  0.3800,  0.3009, -0.1426,  0.2580],\n",
       "        [-0.2083,  0.2290,  0.1755,  0.0593, -0.4879, -0.1924],\n",
       "        [-0.2715, -0.0138,  0.3233,  0.1659, -0.3112, -0.0731],\n",
       "        [-0.6490, -0.3650,  0.3748,  0.3733, -0.1537,  0.2425],\n",
       "        [-0.5734, -0.4600,  0.3751,  0.3776, -0.0236,  0.2299],\n",
       "        [-0.4585, -0.1185,  0.3879,  0.2699, -0.3805,  0.0582],\n",
       "        [-0.6301, -0.4305,  0.4309,  0.3056, -0.0774,  0.1813],\n",
       "        [-0.4581, -0.1763,  0.4155,  0.2491, -0.3281,  0.0634],\n",
       "        [-0.3842, -0.0837,  0.2870,  0.2888, -0.4323, -0.0674],\n",
       "        [-0.2251,  0.2455,  0.1562,  0.1035, -0.4899, -0.1729],\n",
       "        [-0.6539, -0.3418,  0.3664,  0.3597, -0.0616,  0.2559],\n",
       "        [-0.4297, -0.3730,  0.3631,  0.3112, -0.1024,  0.1231],\n",
       "        [-0.5368, -0.4363,  0.4119,  0.2337, -0.0063,  0.1932],\n",
       "        [-0.4507, -0.3828,  0.3510,  0.3873, -0.0372,  0.2956],\n",
       "        [-0.4762, -0.1680,  0.3848,  0.3089, -0.2180,  0.1122],\n",
       "        [-0.3127,  0.0603,  0.2115,  0.1483, -0.4044, -0.1411],\n",
       "        [-0.7163, -0.4405,  0.2895,  0.4370,  0.0208,  0.4317],\n",
       "        [-0.4179, -0.1218,  0.3603,  0.2777, -0.3814, -0.0547],\n",
       "        [-0.2865,  0.0865,  0.2024,  0.2312, -0.5261, -0.1607],\n",
       "        [-0.6000, -0.2751,  0.3607,  0.3465, -0.1723,  0.1742],\n",
       "        [-0.4050, -0.3094,  0.2900,  0.3060, -0.1771,  0.0266],\n",
       "        [-0.4138, -0.1815,  0.3107,  0.3361, -0.3728, -0.0118],\n",
       "        [-0.5152, -0.4049,  0.3261,  0.3413, -0.0615,  0.2338],\n",
       "        [-0.4753, -0.1845,  0.3105,  0.2564, -0.3126, -0.0092],\n",
       "        [-0.4451, -0.2522,  0.4203,  0.3426, -0.2311,  0.0624],\n",
       "        [-0.3299, -0.1315,  0.2351,  0.3039, -0.1995,  0.0824],\n",
       "        [-0.3059, -0.0828,  0.2544,  0.3124, -0.4389, -0.0728],\n",
       "        [-0.3710, -0.1486,  0.3738,  0.2573, -0.2534, -0.0361],\n",
       "        [-0.5520, -0.4463,  0.4208,  0.3482, -0.0567,  0.2355],\n",
       "        [-0.4041, -0.2923,  0.3895,  0.2500, -0.2619,  0.0261],\n",
       "        [-0.3005,  0.0663,  0.2692,  0.1932, -0.4007, -0.0887],\n",
       "        [-0.5589, -0.5228,  0.3479,  0.3852, -0.0248,  0.2595],\n",
       "        [-0.2715,  0.0123,  0.2438,  0.2551, -0.5697, -0.0708]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "batch.pop('labels')\n",
    "lit_teacher_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8366a057-9afd-4ed7-b384-80db9401c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath=\"teacher/\",\n",
    "                                      filename='{epoch}-{val_loss:.2f}-{f1_val_step:.2f}',\n",
    "                                      save_top_k=19,\n",
    "                                      monitor=\"f1_val_step\", mode=\"max\")\n",
    "ea_stop = EarlyStopping(patience=3, monitor=\"f1_val_step\", mode=\"max\")\n",
    "rich = RichProgressBar()\n",
    "trainer = Trainer(callbacks=[checkpoint_callback, ea_stop, rich],accelerator='gpu', \n",
    "                  devices=1, precision=16, val_check_interval=100, check_val_every_n_epoch=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4555711-9523-4420-8824-1e8038a71fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlit_teacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:965\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 965\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/single_device.py:78\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: pl\u001b[38;5;241m.\u001b[39mTrainer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_to_device()\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py:151\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39msetup(trainer)\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_precision_plugin()\n\u001b[1;32m    153\u001b[0m _optimizers_to_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py:140\u001b[0m, in \u001b[0;36mStrategy.setup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler_configs \u001b[38;5;241m=\u001b[39m \u001b[43m_init_optimizers_and_lr_schedulers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py:177\u001b[0m, in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call\n\u001b[0;32m--> 177\u001b[0m optim_conf \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigure_optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optim_conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     rank_zero_warn(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    182\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[9], line 46\u001b[0m, in \u001b[0;36mFTModel.configure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure_optimizers\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:18\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m,\n\u001b[1;32m     15\u001b[0m              weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, amsgrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, foreach: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m              maximize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, capturable: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m              differentiable: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, fused: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid learning rate: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lr))\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m eps:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_teacher_model, train_dl, dev_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73be162e-73c6-4b16-ade4-280bfbeb7a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teacher/epoch=3-val_loss=0.16-f1_val_step=0.92.ckpt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "53aebad2-d67b-4307-a094-d6c5716bb774",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_teacher_model = FTModel.load_from_checkpoint(\"teacher/epoch=3-val_loss=0.16-f1_val_step=0.92.ckpt\", model=model_bert, lr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3c221de7-bc70-43c9-b515-b5b58141e5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cdc337a84e4531bfbfcc2e01372bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       f1_test_step        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8809715509414673     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      f1_test_step       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8809715509414673    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'f1_test_step': 0.8809715509414673}]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(lit_teacher_model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77496f-31bd-4594-95e8-1209cb67c040",
   "metadata": {},
   "source": [
    "## Output our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8c4e5e7f-7b47-4504-8fd3-1e67a7324e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_for_out = DataLoader(data_train, batch_size=64, collate_fn=DataCollatorWithPadding(tokenizer=tokenizer_bert), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "277de683-b95d-4cc0-9bfe-c4dc6418fcb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:15<00:00, 16.28it/s]\n"
     ]
    }
   ],
   "source": [
    "lit_teacher_model.eval()\n",
    "lit_teacher_model = lit_teacher_model.cuda()\n",
    "out_aggregated = []\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(train_dl_for_out):\n",
    "        out = lit_teacher_model(**x.to(\"cuda\"))\n",
    "        out_aggregated.append(out.logits.cpu().detach())\n",
    "out_aggregated = torch.cat(out_aggregated)\n",
    "save_file({'proba': out_aggregated}, \"teacher_proba.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4c2b92d5-609b-417d-ba7c-428403c58e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8089,  0.8797,  4.7062, -1.1240, -1.8250, -1.0949])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_aggregated[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c114f-04f0-403a-b4b3-ef99b6a0b15d",
   "metadata": {},
   "source": [
    "## Our Teacher is 88.09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7bd71-5641-4663-bbae-1d88b5838685",
   "metadata": {},
   "source": [
    "## Train a small model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f130684-9577-4036-85b8-a0ccd9772e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_smol_cfg = deepcopy(model_bert.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec863f28-e525-4422-abec-ac1027158ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_smol_cfg = deepcopy(model_bert.config)\n",
    "model_smol_cfg.num_hidden_layers = 3\n",
    "model_smol_cfg.num_attention_heads = 2\n",
    "small_scratch_model = AutoModelForSequenceClassification.from_config(model_smol_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc213bd8-005a-4cd5-9fc7-19ed619bb52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_small_model = FTModel(small_scratch_model, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de3e3187-0ecb-4a42-9a8c-61960bed070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath=\"student_scratch/\",\n",
    "                                      filename='{epoch}-{val_loss:.2f}-{f1_val_step:.2f}',\n",
    "                                      save_top_k=1,\n",
    "                                      monitor=\"f1_val_step\", mode=\"max\")\n",
    "ea_stop = EarlyStopping(patience=3, monitor=\"f1_val_step\", mode=\"max\")\n",
    "rich = RichProgressBar()\n",
    "trainer = Trainer(callbacks=[checkpoint_callback, ea_stop, rich],accelerator='gpu', \n",
    "                  devices=1, precision=16, val_check_interval=100, check_val_every_n_epoch=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f988546e-a545-469e-a073-37e0fbc76109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model    │ BertForSequenceClassification │ 45.7 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ ce_loss  │ CrossEntropyLoss              │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ train_f1 │ MulticlassF1Score             │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ val_f1   │ MulticlassF1Score             │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ test_f1  │ MulticlassF1Score             │      0 │\n",
       "└───┴──────────┴───────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model    │ BertForSequenceClassification │ 45.7 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ ce_loss  │ CrossEntropyLoss              │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ train_f1 │ MulticlassF1Score             │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ val_f1   │ MulticlassF1Score             │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ test_f1  │ MulticlassF1Score             │      0 │\n",
       "└───┴──────────┴───────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 45.7 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 45.7 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 182                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 45.7 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 45.7 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 182                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2701c267dabf414d99cb8f7d4fe0c8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The \n",
       "'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the \n",
       "`num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The \n",
       "'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the \n",
       "`num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The \n",
       "'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the \n",
       "`num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The \n",
       "'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the \n",
       "`num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lit_small_model, train_dl, dev_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d78c031-84d6-4eb6-8d89-1aa3ab485f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'student_scratch/epoch=5-val_loss=0.28-f1_val_step=0.87.ckpt'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b66a0f8c-6573-4787-aa36-6d38c453e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0849f25c8ea45adb8928ac8d9450895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       f1_test_step        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8355348706245422     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      f1_test_step       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8355348706245422    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'f1_test_step': 0.8355348706245422}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_small_model = FTModel.load_from_checkpoint(checkpoint_callback.best_model_path, model=small_scratch_model, lr=None)\n",
    "trainer.test(lit_small_model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e30d5f-02ba-4586-b621-5e13dab5a3c8",
   "metadata": {},
   "source": [
    "## Output our teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1772f909-769c-4e23-8e8c-5e920da33b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:20<00:00, 11.94it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "692c2fbb-c3f3-4a46-bef2-17b30204769f",
   "metadata": {},
   "source": [
    "## Lets create a KD class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "28bb3c3c-77e7-4f87-9c0a-aef40b3ead24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model, lr: float, weight_default_loss=1, weight_kd_loss=1, teacher_model=None, use_hidden=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_default_loss = weight_default_loss\n",
    "        self.weight_kd_loss = weight_kd_loss\n",
    "        self.ce_loss = CrossEntropyLoss()\n",
    "        self.mse_loss = MSELoss()\n",
    "        self.train_f1 = F1Score('multiclass', num_classes=6, average='macro')\n",
    "        self.val_f1 = F1Score('multiclass', num_classes=6, average='macro')\n",
    "        self.test_f1 = F1Score('multiclass', num_classes=6, average='macro')\n",
    "        self.teacher_model = None\n",
    "        self.use_hidden = use_hidden\n",
    "        if teacher_model is not None and use_hidden:\n",
    "            self.teacher_model = teacher_model\n",
    "            self.teacher_model.eval()\n",
    "            self.teacher_model.requires_grad_(False)\n",
    "            \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        y = batch.pop('labels')\n",
    "        teacher_proba = batch.pop(\"teacher_proba\", default=None)\n",
    "        out = self.model(**batch, output_hidden_states=True)\n",
    "        # 1.21 -0.11 1.44\n",
    "        # 0 0 1\n",
    "        # [GENERATE A JOKE] CHatGPT: [0 0 0 1 0 0 ] is funny jokes\n",
    "        # [GENERATE A JOKE] Student: askdfodsakofdsak\n",
    "        y_hat = out.logits\n",
    "        loss_kd = self.mse_loss(y_hat, teacher_proba)\n",
    "        if self.teacher_model is not None and self.use_hidden:\n",
    "            y_hat_hidden_states = out.hidden_states\n",
    "            out_teacher_model = self.teacher_model(**batch, output_hidden_states=True)\n",
    "            sum_hidden_loss = 0\n",
    "            for std_hid, tch_hid in zip(y_hat_hidden_states, out_teacher_model.hidden_states[::3]):\n",
    "                sum_hidden_loss += self.mse_loss(std_hid, tch_hid)\n",
    "            loss_kd += sum_hidden_loss\n",
    "        loss = self.ce_loss(y_hat,y)\n",
    "        # total_loss = loss * self.weight_default_loss + loss_kd * self.weight_kd_loss\\\n",
    "        total_loss = loss + loss_kd \n",
    "\n",
    "        self.train_f1(y_hat, y)\n",
    "        self.log(\"f1_step\", self.train_f1, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_loss\", total_loss, prog_bar=True)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y = batch.pop('labels')\n",
    "        y_hat = self(**batch).logits\n",
    "        loss = self.ce_loss(y_hat,y)\n",
    "        self.val_f1(y_hat, y)\n",
    "        self.log(\"f1_val_step\", self.val_f1, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y = batch.pop('labels')\n",
    "        y_hat = self(**batch).logits\n",
    "        loss = self.ce_loss(y_hat,y)\n",
    "        self.test_f1(y_hat, y)\n",
    "        self.log(\"f1_test_step\", self.test_f1, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        if 'teacher_model' in state_dict:\n",
    "            state_dict.pop('teacher_model')\n",
    "        checkpoint['state_dict'] = state_dict\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c55aa864-e4a2-4d14-a0a2-fcab3263e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_smol_cfg = deepcopy(model_bert.config)\n",
    "model_smol_cfg.num_hidden_layers = 3\n",
    "model_smol_cfg.num_attention_heads = 2\n",
    "small_scratch_model = AutoModelForSequenceClassification.from_config(model_smol_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b24718eb-65bb-4bff-af10-4ccc7b5ae8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model = KDModel(small_scratch_model, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "756eba40-7979-4e61-9363-7567334d6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_for_kd = data_train.add_column('teacher_proba', out_aggregated.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c6cfe61-8c3c-4566-b5b2-b05ddec9ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_for_kd = DataLoader(data_train_for_kd, batch_size=64, collate_fn=DataCollatorWithPadding(tokenizer=tokenizer_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e66990d2-8cad-4dec-8851-1dbda2223b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath=\"kd_output_label/\",\n",
    "                                      filename='{epoch}-{val_loss:.2f}-{f1_val_step:.2f}',\n",
    "                                      save_top_k=1,\n",
    "                                      monitor=\"f1_val_step\", mode=\"max\")\n",
    "ea_stop = EarlyStopping(patience=5, monitor=\"f1_val_step\", mode=\"max\")\n",
    "rich = RichProgressBar()\n",
    "trainer = Trainer(callbacks=[checkpoint_callback, ea_stop, rich],accelerator='gpu', \n",
    "                  devices=1, precision=16, val_check_interval=100, check_val_every_n_epoch=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8519b42-5c46-4eae-bc46-a50310496b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model    │ BertForSequenceClassification │ 45.7 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ ce_loss  │ CrossEntropyLoss              │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ mse_loss │ MSELoss                       │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_f1 │ MulticlassF1Score             │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_f1   │ MulticlassF1Score             │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_f1  │ MulticlassF1Score             │      0 │\n",
       "└───┴──────────┴───────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model    │ BertForSequenceClassification │ 45.7 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ ce_loss  │ CrossEntropyLoss              │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ mse_loss │ MSELoss                       │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_f1 │ MulticlassF1Score             │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_f1   │ MulticlassF1Score             │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_f1  │ MulticlassF1Score             │      0 │\n",
       "└───┴──────────┴───────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 45.7 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 45.7 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 182                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 45.7 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 45.7 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 182                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942274cab25244e6bb9e625e3ed0b312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(kd_model, train_dl_for_kd, dev_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1508c3c7-1779-4d15-be1e-26bea98ddb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kd_output_label/epoch=7-val_loss=0.32-f1_val_step=0.86.ckpt'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b394a42-3160-474f-abb0-70f280b3bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model = KDModel.load_from_checkpoint(checkpoint_callback.best_model_path, model=small_scratch_model, lr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91471d91-2e73-4744-a469-6c700f064c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5684e3e964894677914e9e34e9a24ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       f1_test_step        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8450269103050232     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      f1_test_step       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8450269103050232    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'f1_test_step': 0.8450269103050232}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(kd_model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da46b6-9451-4a0b-919e-2a38e26cecb1",
   "metadata": {},
   "source": [
    "## Let's use feature matching too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "887b95fe-b53e-48a7-b1e8-121af6b27924",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_smol_cfg = deepcopy(model_bert.config)\n",
    "model_smol_cfg.num_hidden_layers = 3\n",
    "model_smol_cfg.num_attention_heads = 2\n",
    "small_scratch_model = AutoModelForSequenceClassification.from_config(model_smol_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a12b987b-cc69-4eab-8056-72ba8c583630",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model_feature = KDModel(small_scratch_model, 3e-5, teacher_model=lit_teacher_model, use_hidden=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fc4f6710-3b9e-45dc-8010-acb0409395af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath=\"kd_output_label_feature/\",\n",
    "                                      filename='{epoch}-{val_loss:.2f}-{f1_val_step:.2f}',\n",
    "                                      save_top_k=1,\n",
    "                                      monitor=\"f1_val_step\", mode=\"max\")\n",
    "ea_stop = EarlyStopping(patience=5, monitor=\"f1_val_step\", mode=\"max\")\n",
    "rich = RichProgressBar()\n",
    "trainer = Trainer(callbacks=[checkpoint_callback, ea_stop, rich],accelerator='gpu', \n",
    "                  devices=1, precision=16, val_check_interval=100, check_val_every_n_epoch=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "31869d45-cb7e-4bd9-ad2a-814a88fb8b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ BertForSequenceClassification │ 45.7 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ ce_loss       │ CrossEntropyLoss              │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ mse_loss      │ MSELoss                       │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_f1      │ MulticlassF1Score             │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_f1        │ MulticlassF1Score             │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_f1       │ MulticlassF1Score             │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ teacher_model │ FTModel                       │  109 M │\n",
       "└───┴───────────────┴───────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ BertForSequenceClassification │ 45.7 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ ce_loss       │ CrossEntropyLoss              │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ mse_loss      │ MSELoss                       │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_f1      │ MulticlassF1Score             │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_f1        │ MulticlassF1Score             │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_f1       │ MulticlassF1Score             │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ teacher_model │ FTModel                       │  109 M │\n",
       "└───┴───────────────┴───────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 45.7 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 109 M                                                                                        \n",
       "<span style=\"font-weight: bold\">Total params</span>: 155 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 620                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 45.7 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 109 M                                                                                        \n",
       "\u001b[1mTotal params\u001b[0m: 155 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 620                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ba39e59e474a1c8906b840eea57b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(kd_model_feature, train_dl_for_kd, dev_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c8c973ce-0b30-4a60-a9a8-3ac179bba8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model_feature = KDModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, model=small_scratch_model, lr=3e-5, teacher_model=lit_teacher_model, use_hidden=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c9769600-c3b7-4c86-b23b-eb3362159069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6764a0a6612940d687aff808c9623cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       f1_test_step        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8488346338272095     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      f1_test_step       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8488346338272095    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'f1_test_step': 0.8488346338272095}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(kd_model_feature, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5334a-4a40-4ef8-a1d7-d0196c04f59e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
